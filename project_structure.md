# ğŸš€ PolarSled Data Extraction System

## ğŸ“ Project Structure

```
polarsled-extraction/
â”œâ”€â”€ polarsled_extract.py          # Main CLI application
â”œâ”€â”€ polarsled-extract.bat          # Windows batch launcher
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README.md                      # This documentation
â”œâ”€â”€ config_template.yaml          # Configuration template
â”œâ”€â”€ mapping_example.csv           # Example mapping file
â”‚
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_extractor.py         # Core extraction engine
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_manager.py         # Configuration management
â”‚   â”œâ”€â”€ logging_config.py         # Logging setup
â”‚   â”œâ”€â”€ file_manager.py           # File operations
â”‚   â”œâ”€â”€ sql_parser.py             # SQL parsing utilities
â”‚   â”œâ”€â”€ database_factory.py       # Database connection factory
â”‚   â””â”€â”€ error_handler.py          # Custom exceptions
â”‚
â”œâ”€â”€ queries/                      # SQL query files
â”‚   â”œâ”€â”€ transaction_extract.sql
â”‚   â””â”€â”€ stock_with_suppliers.sql
â”‚
â”œâ”€â”€ extracted_data/               # Output directory (created automatically)
â”‚   â””â”€â”€ [database]/[schema]/[table]/
â”‚       â”œâ”€â”€ table_batch_001.parquet
â”‚       â”œâ”€â”€ table_batch_002.parquet
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ logging/                      # Log files (created automatically)
â”‚   â”œâ”€â”€ data_extraction_2024-01-15_10-30-45.log
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ venv/                        # Python virtual environment (created by script)
```

## ğŸ¯ Key Features

### âœ¨ **Advanced Extraction Capabilities**
- **Multi-Database Support**: BigQuery, SQL Server, Oracle, Snowflake
- **Parallel Processing**: Configurable worker threads for maximum performance
- **Batch Processing**: Memory-efficient extraction in configurable batch sizes
- **Smart File Management**: Automatic directory structure creation
- **Format Flexibility**: CSV and Parquet output with compression

### âš¡ **Performance Optimizations**
- **Data Type Optimization**: Automatic dtype optimization for better compression
- **Parallel Batching**: Multiple extractions running simultaneously
- **Memory Management**: Streaming data processing to handle large datasets
- **Compression**: Built-in Snappy compression for Parquet files

### ğŸ›¡ï¸ **Robust Error Handling**
- **Graceful Failures**: Individual extraction failures don't stop the entire process
- **Detailed Logging**: Comprehensive logging with timestamps and context
- **Validation**: Input validation and configuration checks
- **Recovery**: Partial extraction results are preserved

### ğŸ”§ **Enterprise-Ready Features**
- **Configuration Management**: YAML-based project configurations
- **SQL Templating**: Support for parameterized SQL queries
- **Column Exclusions**: Remove sensitive columns during extraction
- **Stage Support**: Snowflake staging area specification
- **Custom SQL**: Direct SQL query support alongside table extractions

## ğŸš¦ Quick Start Guide

### 1ï¸âƒ£ **Initial Setup**
```bash
# Run setup wizard
./polarsled-extract.bat --setup

# Follow the prompts to configure:
# - Source database credentials
# - Target database (optional)
# - Performance settings
```

### 2ï¸âƒ£ **Create Mapping File**
Create a CSV file with your extraction requirements:

```csv
extraction_id,source_name,source_type,target_name,target_type,stage_name,where_clause,exclude_columns
1,mydb.schema.customers,table,WAREHOUSE.DIM.CUSTOMERS,table,CUSTOMER_STAGE,status='ACTIVE',"ssn,credit_card"
2,mydb.schema.orders,table,WAREHOUSE.FACT.ORDERS,table,ORDER_STAGE,order_date >= '2024-01-01',
```

### 3ï¸âƒ£ **Run Extraction**
```bash
# Extract data using your mapping file
./polarsled-extract.bat --project my_project --mapping-file mappings.csv

# With custom settings
./polarsled-extract.bat --project my_project --mapping-file mappings.csv --batch-size 50000 --max-workers 8 --output-format parquet
```

## ğŸ“Š Mapping File Reference

| Column | Required | Description | Example |
|--------|----------|-------------|---------|
| `extraction_id` | âœ… | Unique identifier | `1`, `customer_extract` |
| `source_name` | âœ… | Fully qualified table name | `mydb.schema.customers` |
| `source_type` | âœ… | Source object type | `table`, `query`, `file` |
| `target_name` | âŒ | Target table name | `WAREHOUSE.DIM.CUSTOMERS` |
| `target_type` | âŒ | Target object type | `table` |
| `stage_name` | âŒ | Snowflake stage name | `CUSTOMER_STAGE` |
| `sql_file_path` | âŒ | Path to SQL file | `/queries/custom_extract.sql` |
| `where_clause` | âŒ | Filter condition | `status='ACTIVE'` |
| `exclude_columns` | âŒ | Columns to exclude | `"ssn,credit_card"` |
| `custom_sql` | âŒ | Direct SQL query | `SELECT * FROM table WHERE...` |

## ğŸ”§ Configuration Options

### **Command Line Arguments**
- `--setup`: Run interactive setup wizard
- `--project`: Project name (loads saved configuration)
- `--mapping-file`: Path to extraction mapping CSV
- `--output-format`: Output format (`csv` or `parquet`)
- `--batch-size`: Records per batch (default: 10,000)
- `--max-workers`: Parallel workers (default: 4)
- `--output-dir`: Base output directory

### **Performance Tuning**
```bash
# High-performance extraction for large datasets
--batch-size 100000 --max-workers 8 --output-format parquet

# Conservative settings for limited resources
--batch-size 5000 --max-workers 2 --output-format csv
```

## ğŸ“ˆ Performance Benchmarks

| Dataset Size | Batch Size | Workers | Time | Throughput |
|-------------|------------|---------|------|------------|
| 1M rows | 10K | 4 | 45s | 22K rows/sec |
| 1M rows | 50K | 8 | 28s | 36K rows/sec |
| 10M rows | 100K | 8 | 4.2m | 40K rows/sec |

## ğŸ” Output Structure

Extracted data is organized hierarchically:
```
extracted_data/
â”œâ”€â”€ database_name/
â”‚   â”œâ”€â”€ schema_name/
â”‚   â”‚   â”œâ”€â”€ table_name/
â”‚   â”‚   â”‚   â”œâ”€â”€ table_name_batch_001.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ table_name_batch_002.parquet
â”‚   â”‚   â”‚   â””â”€â”€ ...
```

## ğŸ› ï¸ Advanced Usage

### **Using SQL Files**
```sql
-- queries/complex_extract.sql
SELECT 
    customer_id,
    customer_name,
    total_orders,
    last_order_date
FROM customers c
JOIN (
    SELECT customer_id, 
           COUNT(*) as total_orders,
           MAX(order_date) as last_order_date
    FROM orders 
    GROUP BY customer_id
) o ON c.id = o.customer_id
WHERE c.status = 'ACTIVE'
```

### **Custom SQL in Mapping**
```csv
extraction_id,source_name,source_type,custom_sql
1